{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from langchain_elasticsearch import ElasticsearchRetriever\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing import Dict\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import (\n",
    "    ChatGoogleGenerativeAI,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ppathorn/Documents/GitHub/llm_zoomcamp_2024_project/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=model_name)\n",
    "es_url = 'http://localhost:9200'\n",
    "def hybrid_query(query: str) -> Dict:\n",
    "    vector = embeddings.embed_query(query)  # same embeddings as for indexing\n",
    "    return {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fuzziness\": \"AUTO\",\n",
    "                        \"fields\": [\"content\", \"title\",\"description\"],\n",
    "                        \"type\": \"best_fields\",\n",
    "                        \"boost\": 0.5,\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"knn\": {\n",
    "            \"field\": \"content_vector\",\n",
    "            \"query_vector\": vector,\n",
    "            \"k\": 3,\n",
    "            \"num_candidates\": 10000,\n",
    "            \"boost\": 0.5,\n",
    "        },\n",
    "        \"size\": 3,\n",
    "    }\n",
    "\n",
    "index_name = \"relationship_consult\"\n",
    "hybrid_retriever = ElasticsearchRetriever.from_es_params(\n",
    "    index_name=index_name,\n",
    "    body_func=hybrid_query,\n",
    "    content_field='content',\n",
    "    url=es_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ทำความรู้จักกับ โรคซึมเศร้า โดยจิตแพทย์ - Alljit Blog 2.001563\n",
      "แฟนป่วยโรคซึมเศร้า ส่วนเราเอาไงดี? รับมืออย่างไร - Alljit Blog 1.9367619\n",
      "มีแฟนแต่รู้สึกเหงา นักจิตวิทยามองว่าอย่างไร? - Alljit Blog 0.36788744\n"
     ]
    }
   ],
   "source": [
    "query = \"เศร้า\"\n",
    "hybrid_results = hybrid_retriever.invoke(query)\n",
    "for result in hybrid_results:\n",
    "    print(result.metadata['_source']['title'], result.metadata['_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>แฟนที่เคยแสนดี นอกใจ มาขอโอกาส นักจิตวิทยาช่วย...</td>\n",
       "      <td>relationship</td>\n",
       "      <td>เมื่อแฟนที่เคยแสนดี นอกใจ มาขอโอกาส เราควรพิจา...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>แฟนที่เคยแสนดี นอกใจ มาขอโอกาส นักจิตวิทยาช่วย...</td>\n",
       "      <td>relationship</td>\n",
       "      <td>การให้โอกาสแฟนที่นอกใจหมายถึงอะไร? มันแตกต่างจ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>แฟนที่เคยแสนดี นอกใจ มาขอโอกาส นักจิตวิทยาช่วย...</td>\n",
       "      <td>relationship</td>\n",
       "      <td>ทำไมเราถึงรู้สึกอยากให้โอกาสแฟนที่นอกใจ แม้จะร...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>แฟนที่เคยแสนดี นอกใจ มาขอโอกาส นักจิตวิทยาช่วย...</td>\n",
       "      <td>relationship</td>\n",
       "      <td>การให้โอกาสแฟนที่นอกใจอาจนำไปสู่ผลลัพธ์อะไรบ้าง?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>แฟนที่เคยแสนดี นอกใจ มาขอโอกาส นักจิตวิทยาช่วย...</td>\n",
       "      <td>relationship</td>\n",
       "      <td>เราจะรู้ได้อย่างไรว่าการให้โอกาสแฟนที่นอกใจเป็...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title       section  \\\n",
       "0  แฟนที่เคยแสนดี นอกใจ มาขอโอกาส นักจิตวิทยาช่วย...  relationship   \n",
       "1  แฟนที่เคยแสนดี นอกใจ มาขอโอกาส นักจิตวิทยาช่วย...  relationship   \n",
       "2  แฟนที่เคยแสนดี นอกใจ มาขอโอกาส นักจิตวิทยาช่วย...  relationship   \n",
       "3  แฟนที่เคยแสนดี นอกใจ มาขอโอกาส นักจิตวิทยาช่วย...  relationship   \n",
       "4  แฟนที่เคยแสนดี นอกใจ มาขอโอกาส นักจิตวิทยาช่วย...  relationship   \n",
       "\n",
       "                                            question  \n",
       "0  เมื่อแฟนที่เคยแสนดี นอกใจ มาขอโอกาส เราควรพิจา...  \n",
       "1  การให้โอกาสแฟนที่นอกใจหมายถึงอะไร? มันแตกต่างจ...  \n",
       "2  ทำไมเราถึงรู้สึกอยากให้โอกาสแฟนที่นอกใจ แม้จะร...  \n",
       "3   การให้โอกาสแฟนที่นอกใจอาจนำไปสู่ผลลัพธ์อะไรบ้าง?  \n",
       "4  เราจะรู้ได้อย่างไรว่าการให้โอกาสแฟนที่นอกใจเป็...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df=pd.read_csv('ground_truth.csv')\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved_results, relevant_results, k):\n",
    "    retrieved_at_k = retrieved_results[:k]\n",
    "    relevant_at_k = [1 if doc in relevant_results else 0 for doc in retrieved_at_k]\n",
    "    return sum(relevant_at_k) / k\n",
    "\n",
    "def recall(retrieved_results, relevant_results):\n",
    "    relevant_retrieved = [1 if doc in relevant_results else 0 for doc in retrieved_results]\n",
    "    return sum(relevant_retrieved) / len(relevant_results)\n",
    "\n",
    "def average_precision(retrieved_results, relevant_results):\n",
    "    relevant_retrieved = [1 if doc in relevant_results else 0 for doc in retrieved_results]\n",
    "    precisions = [precision_at_k(retrieved_results, relevant_results, k+1) for k in range(len(relevant_retrieved)) if relevant_retrieved[k] == 1]\n",
    "    if len(precisions) == 0:\n",
    "        return 0\n",
    "    return sum(precisions) / len(relevant_results)\n",
    "\n",
    "def mean_average_precision(retrieved_results_list, relevant_results_list):\n",
    "    return sum([average_precision(retrieved_results, relevant_results) \n",
    "                for retrieved_results, relevant_results in zip(retrieved_results_list, relevant_results_list)]) / len(relevant_results_list)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Discounted Cumulative Gain (DCG)\n",
    "def dcg(relevances, p):\n",
    "    \"\"\"\n",
    "    Compute DCG for the given relevances and position p.\n",
    "    relevances: List of binary relevance scores (1 for relevant, 0 for non-relevant)\n",
    "    p: The number of top results to consider for DCG\n",
    "    \"\"\"\n",
    "    return sum((2**relevances[i] - 1) / np.log2(i + 2) for i in range(min(len(relevances), p)))\n",
    "\n",
    "# Normalized Discounted Cumulative Gain (NDCG)\n",
    "def ndcg(retrieved_results, relevant_results, p):\n",
    "    \"\"\"\n",
    "    Compute NDCG at position p.\n",
    "    retrieved_results: List of retrieved document titles\n",
    "    relevant_results: List of relevant document titles\n",
    "    p: The cutoff position for evaluation (e.g., NDCG@p)\n",
    "    \"\"\"\n",
    "    # Calculate relevance scores for the retrieved results (1 if relevant, 0 if not)\n",
    "    relevances = [1 if doc in relevant_results else 0 for doc in retrieved_results]\n",
    "    \n",
    "    # Compute DCG for the retrieved results\n",
    "    dcg_value = dcg(relevances, p)\n",
    "    \n",
    "    # Create ideal relevance ordering (all relevant documents ranked first)\n",
    "    ideal_relevances = sorted(relevances, reverse=True)\n",
    "    \n",
    "    # Compute ideal DCG (IDCG)\n",
    "    idcg_value = dcg(ideal_relevances, p)\n",
    "    \n",
    "    # To avoid division by zero, return 0 if IDCG is 0\n",
    "    if idcg_value == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Compute NDCG by normalizing DCG with IDCG\n",
    "    return dcg_value / idcg_value\n",
    "\n",
    "\n",
    "def reciprocal_rank(retrieved_results, relevant_results):\n",
    "    for i, doc in enumerate(retrieved_results):\n",
    "        if doc in relevant_results:\n",
    "            return 1 / (i + 1)\n",
    "    return 0\n",
    "\n",
    "def mean_reciprocal_rank(retrieved_results_list, relevant_results_list):\n",
    "    return sum([reciprocal_rank(retrieved_results, relevant_results)\n",
    "                for retrieved_results, relevant_results in zip(retrieved_results_list, relevant_results_list)]) / len(relevant_results_list)\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auc_roc(retrieved_relevances, true_relevances):\n",
    "    return roc_auc_score(true_relevances, retrieved_relevances)\n",
    "\n",
    "\n",
    "def hit_rate_at_k(retrieved_results, relevant_results, k):\n",
    "    retrieved_at_k = retrieved_results[:k]\n",
    "    return 1 if any(doc in relevant_results for doc in retrieved_at_k) else 0\n",
    "\n",
    "def hit_rate(retrieved_results_list, relevant_results_list, k):\n",
    "    return sum([hit_rate_at_k(retrieved_results, relevant_results, k)\n",
    "                for retrieved_results, relevant_results in zip(retrieved_results_list, relevant_results_list)]) / len(relevant_results_list)\n",
    "\n",
    "\n",
    "def err(retrieved_results, relevant_results):\n",
    "    err_value = 0\n",
    "    relevance_probability = 1\n",
    "    for i, doc in enumerate(retrieved_results):\n",
    "        relevance = 1 if doc in relevant_results else 0\n",
    "        err_value += relevance_probability * (relevance / (i + 1))\n",
    "        relevance_probability *= (1 - relevance)\n",
    "    return err_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@3: 0.23715651135005864\n",
      "Average Recall: 0.7114695340501792\n",
      "Average MAP: 0.6439665471923534\n",
      "Average NDCG@3: 0.6613311266385073\n",
      "Average MRR: 0.6439665471923534\n",
      "Average F1 Score: 0.3557347670250896\n",
      "Average Hit Rate@3: 0.7114695340501792\n",
      "Average ERR: 0.6439665471923534\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each question in ground_truth\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "map_scores = []\n",
    "ndcg_scores = []\n",
    "mrr_scores = []\n",
    "f1_scores = []\n",
    "hit_rate_scores = []\n",
    "err_scores = []\n",
    "\n",
    "for i, row in eval_df.iterrows():\n",
    "    question = row['question']\n",
    "    \n",
    "    # Use the question as the query in the hybrid retriever\n",
    "    hybrid_results = hybrid_retriever.invoke(question)\n",
    "    \n",
    "    # Extract the retrieved titles\n",
    "    retrieved_titles = [result.metadata['_source']['title'] for result in hybrid_results]\n",
    "    \n",
    "    # Define the relevant title (from the ground_truth row)\n",
    "    relevant_titles = [row['title']]\n",
    "    \n",
    "    # Calculate Precision@k, Recall, and MAP\n",
    "    precision_k = precision_at_k(retrieved_titles, relevant_titles, k=3)\n",
    "    recall_score = recall(retrieved_titles, relevant_titles)\n",
    "    map_score = average_precision(retrieved_titles, relevant_titles)\n",
    "    \n",
    "    # Calculate NDCG@k (assuming k=3)\n",
    "    ndcg_score = ndcg(retrieved_titles, relevant_titles, p=3)\n",
    "    \n",
    "    # Calculate Reciprocal Rank (for MRR)\n",
    "    mrr_score = reciprocal_rank(retrieved_titles, relevant_titles)\n",
    "    \n",
    "    # Calculate F1 Score\n",
    "    f1 = f1_score(precision_k, recall_score)\n",
    "    \n",
    "    # Calculate Hit Rate@k (k=3)\n",
    "    hit_rate_score = hit_rate_at_k(retrieved_titles, relevant_titles, k=3)\n",
    "    \n",
    "    # Calculate ERR\n",
    "    err_score = err(retrieved_titles, relevant_titles)\n",
    "    \n",
    "    # Append results for each query\n",
    "    precision_scores.append(precision_k)\n",
    "    recall_scores.append(recall_score)\n",
    "    map_scores.append(map_score)\n",
    "    ndcg_scores.append(ndcg_score)\n",
    "    mrr_scores.append(mrr_score)\n",
    "    f1_scores.append(f1)\n",
    "    hit_rate_scores.append(hit_rate_score)\n",
    "    err_scores.append(err_score)\n",
    "\n",
    "# Calculate overall average metrics\n",
    "avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "avg_map = sum(map_scores) / len(map_scores)\n",
    "avg_ndcg = sum(ndcg_scores) / len(ndcg_scores)\n",
    "avg_mrr = sum(mrr_scores) / len(mrr_scores)\n",
    "avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "avg_hit_rate = sum(hit_rate_scores) / len(hit_rate_scores)\n",
    "avg_err = sum(err_scores) / len(err_scores)\n",
    "\n",
    "# Print out the final metrics\n",
    "print(f\"Average Precision@3: {avg_precision}\")\n",
    "print(f\"Average Recall: {avg_recall}\")\n",
    "print(f\"Average MAP: {avg_map}\")\n",
    "print(f\"Average NDCG@3: {avg_ndcg}\")\n",
    "print(f\"Average MRR: {avg_mrr}\")\n",
    "print(f\"Average F1 Score: {avg_f1}\")\n",
    "print(f\"Average Hit Rate@3: {avg_hit_rate}\")\n",
    "print(f\"Average ERR: {avg_err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Search Hybri Search Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "| Metric                 | Score         |\n",
    "|------------------------|---------------|\n",
    "| **Average Precision@3** | 0.2389        |\n",
    "| **Average Recall**      | 0.7168        |\n",
    "| **Average MAP**         | 0.6478        |\n",
    "| **Average NDCG@3**      | 0.6656        |\n",
    "| **Average MRR**         | 0.6478        |\n",
    "| **Average F1 Score**    | 0.3584        |\n",
    "| **Average Hit Rate@3**  | 0.7168        |\n",
    "| **Average ERR**         | 0.6478        |\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **Precision@3 (23.89%)**: The system retrieves only a small fraction of relevant documents in the top 3 positions, indicating low precision.\n",
    "- **Recall (71.68%)**: The system does a good job of retrieving most relevant documents, but some are still missed.\n",
    "- **Mean Average Precision (MAP 64.78%)**: Relevant documents tend to be found early in the results, which is a positive indicator of system performance.\n",
    "- **NDCG@3 (66.56%)**: The system ranks the documents **66.56% as effectively as an ideal ranking**, showing it’s fairly good at ordering results by relevance.\n",
    "- **MRR (64.78%)**: On average, the first relevant document is found relatively high in the list.\n",
    "- **F1 Score (35.84%)**: Although recall is high, low precision results in a lower F1 score, indicating a need for improvement in reducing irrelevant results.\n",
    "- **Hit Rate@3 (71.68%)**: For about **71.68% of the queries**, at least one relevant document is found within the top 3 results.\n",
    "- **ERR (64.78%)**: Users are likely to find relevant documents without needing to go far down the results, as **64.78% of the first relevant documents are found early** in the list.\n",
    "\n",
    "### Conclusion:\n",
    "The system performs well in terms of recall (finding most relevant documents) and ranks relevant documents relatively high. However, there’s room for improvement in terms of precision, as the top results often include irrelevant documents. Improving the precision would also improve the F1 score and overall user satisfaction with the ranking of results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma DB Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key='xxxx'\n",
    "doc_embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\", task_type=\"retrieval_document\",google_api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"relationship\",\n",
    "    embedding_function=doc_embeddings,\n",
    "    persist_directory=\"vector_stores\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"k\": 3, \"fetch_k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=retriever.invoke(\"เศร้า\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'รักต่างวัย กับ จิตวิทยาความรัก - Alljit Blog'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].metadata['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@3: 0.03584229390681003\n",
      "Average Recall: 0.10752688172043011\n",
      "Average MAP: 0.08811230585424135\n",
      "Average NDCG@3: 0.09129072990911476\n",
      "Average MRR: 0.08632019115890084\n",
      "Average F1 Score: 0.053763440860215055\n",
      "Average Hit Rate@3: 0.1057347670250896\n",
      "Average ERR: 0.08632019115890084\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each question in ground_truth\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "map_scores = []\n",
    "ndcg_scores = []\n",
    "mrr_scores = []\n",
    "f1_scores = []\n",
    "hit_rate_scores = []\n",
    "err_scores = []\n",
    "\n",
    "for i, row in eval_df.iterrows():\n",
    "    question = row['question']\n",
    "    \n",
    "    # Use the question as the query in the hybrid retriever\n",
    "    results = retriever.invoke(question)\n",
    "    \n",
    "    # Extract the retrieved titles\n",
    "    retrieved_titles = [result.metadata['title'] for result in results]\n",
    "    \n",
    "    # Define the relevant title (from the ground_truth row)\n",
    "    relevant_titles = [row['title']]\n",
    "    \n",
    "    # Calculate Precision@k, Recall, and MAP\n",
    "    precision_k = precision_at_k(retrieved_titles, relevant_titles, k=3)\n",
    "    recall_score = recall(retrieved_titles, relevant_titles)\n",
    "    map_score = average_precision(retrieved_titles, relevant_titles)\n",
    "    \n",
    "    # Calculate NDCG@k (assuming k=3)\n",
    "    ndcg_score = ndcg(retrieved_titles, relevant_titles, p=3)\n",
    "    \n",
    "    # Calculate Reciprocal Rank (for MRR)\n",
    "    mrr_score = reciprocal_rank(retrieved_titles, relevant_titles)\n",
    "    \n",
    "    # Calculate F1 Score\n",
    "    f1 = f1_score(precision_k, recall_score)\n",
    "    \n",
    "    # Calculate Hit Rate@k (k=3)\n",
    "    hit_rate_score = hit_rate_at_k(retrieved_titles, relevant_titles, k=3)\n",
    "    \n",
    "    # Calculate ERR\n",
    "    err_score = err(retrieved_titles, relevant_titles)\n",
    "    \n",
    "    # Append results for each query\n",
    "    precision_scores.append(precision_k)\n",
    "    recall_scores.append(recall_score)\n",
    "    map_scores.append(map_score)\n",
    "    ndcg_scores.append(ndcg_score)\n",
    "    mrr_scores.append(mrr_score)\n",
    "    f1_scores.append(f1)\n",
    "    hit_rate_scores.append(hit_rate_score)\n",
    "    err_scores.append(err_score)\n",
    "\n",
    "# Calculate overall average metrics\n",
    "avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "avg_map = sum(map_scores) / len(map_scores)\n",
    "avg_ndcg = sum(ndcg_scores) / len(ndcg_scores)\n",
    "avg_mrr = sum(mrr_scores) / len(mrr_scores)\n",
    "avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "avg_hit_rate = sum(hit_rate_scores) / len(hit_rate_scores)\n",
    "avg_err = sum(err_scores) / len(err_scores)\n",
    "\n",
    "# Print out the final metrics\n",
    "print(f\"Average Precision@3: {avg_precision}\")\n",
    "print(f\"Average Recall: {avg_recall}\")\n",
    "print(f\"Average MAP: {avg_map}\")\n",
    "print(f\"Average NDCG@3: {avg_ndcg}\")\n",
    "print(f\"Average MRR: {avg_mrr}\")\n",
    "print(f\"Average F1 Score: {avg_f1}\")\n",
    "print(f\"Average Hit Rate@3: {avg_hit_rate}\")\n",
    "print(f\"Average ERR: {avg_err}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
