{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UgXXMOQAxMcM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PlaywrightURLLoader\n",
        "from langchain_community.document_loaders import SeleniumURLLoader\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "from urllib.parse import urlparse\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0YB5Ua5ZxQ9A"
      },
      "outputs": [],
      "source": [
        "async def get_blog_urls(base_url:str):\n",
        "  loader=AsyncChromiumLoader([base_url])\n",
        "  transform=Html2TextTransformer()\n",
        "  docs=await loader.aload()\n",
        "  html_content = docs[0].page_content\n",
        "  return html_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_content(content):\n",
        "    # Step 1: Replace newline characters and non-breaking spaces\n",
        "    content = content.replace('\\n', '').replace('\\xa0', ' ')\n",
        "\n",
        "    # Step 2: Use regex to remove multiple spaces and strip extra spaces at the beginning and end\n",
        "    content = re.sub(r'\\s+', ' ', content).strip()\n",
        "\n",
        "    # Step 3: Remove everything starting from 'Post Views' until the end of the content\n",
        "    content = re.sub(r'Post Views:.*', '', content, flags=re.DOTALL)\n",
        "\n",
        "    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2KLnW3504H4"
      },
      "source": [
        "#### Run within notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRhWMrdvy0Jh",
        "outputId": "02bb6c8f-f9d1-4416-cd15-1db5e66d42e0"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://www.mind.org.uk/information-support/types-of-mental-health-problems/\"\n",
        "docs = await get_blog_urls(base_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSHN9qk_07QZ"
      },
      "source": [
        "#### Run outside notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y87jLR17083x"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "base_url = \"https://www.mind.org.uk/information-support/types-of-mental-health-problems/\"\n",
        "docs = asyncio.run(get_blog_urls(base_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHe2xmIil2g2"
      },
      "source": [
        "## Mental Issue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaowRY6gVwcn",
        "outputId": "da067a3b-a901-4ced-f7ae-bef865dfb425"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://www.mind.org.uk/information-support/types-of-mental-health-problems/\"\n",
        "html_content = await get_blog_urls(base_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnGwp5-mfjx_"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "hrefs = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "filtered_urls = [href for href in hrefs if href.startswith('/information-support/types-of-mental-health-problems/')]\n",
        "filtered_urls=filtered_urls[1:]\n",
        "base_url = \"https://www.mind.org.uk\"\n",
        "mental_issue_urls = [base_url + url for url in filtered_urls]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnzFIoDQmIha"
      },
      "source": [
        "## Mental tips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQpLQM3XmrzR",
        "outputId": "a3626f60-a503-4d56-f885-614ab0c97392"
      },
      "outputs": [],
      "source": [
        "base_url=\"https://www.mind.org.uk/information-support/tips-for-everyday-living/\"\n",
        "html_content = await get_blog_urls(base_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAPATVVtmyIu"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "hrefs = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "filtered_urls = [href for href in hrefs if href.startswith('/information-support/tips-for-everyday-living/')]\n",
        "filtered_urls=filtered_urls[1:]\n",
        "base_url = \"https://www.mind.org.uk\"\n",
        "tips_urls = [base_url + url for url in filtered_urls]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCtaF9x1tio2"
      },
      "source": [
        "## Dating Thai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA_zRKQi13BJ",
        "outputId": "86f6792b-324b-46f0-fc7b-28a688e0f9b1"
      },
      "outputs": [],
      "source": [
        "#Find last page\n",
        "base_url=f\"https://www.alljitblog.com/category/จิตวิทยาชีวิตคู่/\"\n",
        "html_content = await get_blog_urls(base_url)\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Find the pagination section\n",
        "pagination = soup.find('div', class_='box-pagination')\n",
        "if pagination:\n",
        "    # Extract all page numbers from the pagination section\n",
        "    pages = pagination.find_all('a', class_='page-numbers')\n",
        "\n",
        "    # Convert to integers and find the largest number\n",
        "    last_page = max([int(page.text) for page in pages if page.text.isdigit()])\n",
        "\n",
        "    print(f\"The last page number is: {last_page}\")\n",
        "else:\n",
        "    print(\"Pagination not found.\")\n",
        "\n",
        "\n",
        "#Find all blog url\n",
        "dating_th_urls=[]\n",
        "for i in tqdm(range(1, last_page+1)):\n",
        "  base_url=f\"https://www.alljitblog.com/category/จิตวิทยาชีวิตคู่/page/{i}/\"\n",
        "  html_content = await get_blog_urls(base_url)\n",
        "  soup = BeautifulSoup(html_content, 'html.parser')\n",
        "  hrefs = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "  filtered_urls = [href for href in hrefs if href.startswith('https://www.alljitblog.com/')]\n",
        "  filtered_urls=filtered_urls[1:]\n",
        "  filtered_urls= list(set(filtered_urls))\n",
        "  unwanted_substrings = ['?cat', '.com/#', '/author/admin-alljit/','/category/']\n",
        "  filtered_urls=[url for url in filtered_urls if not any(substring in url for substring in unwanted_substrings)]\n",
        "  dating_th_urls.extend(filtered_urls)\n",
        "\n",
        "\n",
        "dating_th_urls = list({urlparse(url).scheme + '://' + urlparse(url).netloc + urlparse(url).path for url in dating_th_urls})\n",
        "dating_th_urls.remove('https://www.alljitblog.com/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B-rl2xd99tPi"
      },
      "outputs": [],
      "source": [
        "loader_multiple_pages = WebBaseLoader(dating_th_urls,encoding = 'utf-8')\n",
        "dating_data = loader_multiple_pages.load()\n",
        "for data in dating_data:\n",
        "  data.page_content = data.page_content.replace('\\n', '')\n",
        "  data.page_content = data.page_content.replace('\\xa0', ' ')\n",
        "  data.page_content = re.sub(r'\\s+', ' ', data.page_content).strip()\n",
        "  data.page_content = clean_content(data.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A45qoBmsAlrb"
      },
      "source": [
        "## Psychiatrist Thai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9j3qREmA0uM",
        "outputId": "aadff70e-57c8-4b23-e7fb-40a445e38161"
      },
      "outputs": [],
      "source": [
        "#Find last page\n",
        "base_url=f\"https://www.alljitblog.com/category/psychiatrist/\"\n",
        "html_content = await get_blog_urls(base_url)\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Find the pagination section\n",
        "pagination = soup.find('div', class_='box-pagination')\n",
        "if pagination:\n",
        "    # Extract all page numbers from the pagination section\n",
        "    pages = pagination.find_all('a', class_='page-numbers')\n",
        "\n",
        "    # Convert to integers and find the largest number\n",
        "    last_page = max([int(page.text) for page in pages if page.text.isdigit()])\n",
        "\n",
        "    print(f\"The last page number is: {last_page}\")\n",
        "else:\n",
        "    print(\"Pagination not found.\")\n",
        "\n",
        "\n",
        "#Find all blog url\n",
        "psy_th_urls=[]\n",
        "for i in tqdm(range(1, last_page+1)):\n",
        "  base_url=f\"https://www.alljitblog.com/category/psychiatrist/page/{i}/\"\n",
        "  html_content = await get_blog_urls(base_url)\n",
        "  soup = BeautifulSoup(html_content, 'html.parser')\n",
        "  hrefs = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "  filtered_urls = [href for href in hrefs if href.startswith('https://www.alljitblog.com/')]\n",
        "  filtered_urls=filtered_urls[1:]\n",
        "  filtered_urls= list(set(filtered_urls))\n",
        "  unwanted_substrings = ['?cat', '.com/#', '/author/admin-alljit/','/category/']\n",
        "  filtered_urls=[url for url in filtered_urls if not any(substring in url for substring in unwanted_substrings)]\n",
        "  psy_th_urls.extend(filtered_urls)\n",
        "\n",
        "psy_th_urls = list({urlparse(url).scheme + '://' + urlparse(url).netloc + urlparse(url).path for url in psy_th_urls})\n",
        "psy_th_urls.remove('https://www.alljitblog.com/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CVYV5NKIAKUH"
      },
      "outputs": [],
      "source": [
        "loader_multiple_pages = WebBaseLoader(psy_th_urls,encoding = 'utf-8')\n",
        "psy_data = loader_multiple_pages.load()\n",
        "for data in psy_data:\n",
        "  data.page_content = data.page_content.replace('\\n', '')\n",
        "  data.page_content = data.page_content.replace('\\xa0', ' ')\n",
        "  data.page_content = re.sub(r'\\s+', ' ', data.page_content).strip()\n",
        "  data.page_content = clean_content(data.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data prep to elastic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ppathorn/Documents/GitHub/llm_zoomcamp_2024_project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "from langchain_elasticsearch import ElasticsearchRetriever\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from typing import Dict\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_google_genai import (\n",
        "    ChatGoogleGenerativeAI,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        ")\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "es_client = Elasticsearch('http://localhost:9200') \n",
        "\n",
        "index_settings = {\n",
        "    \"settings\": {\n",
        "        \"number_of_shards\": 1,\n",
        "        \"number_of_replicas\": 0\n",
        "    },\n",
        "    \"mappings\": {\n",
        "        \"properties\": {\n",
        "            \"source\": {\"type\": \"text\"},\n",
        "            \"title\": {\"type\": \"text\"},\n",
        "            \"description\": {\"type\": \"text\"},\n",
        "            \"section\": {\"type\": \"keyword\"},\n",
        "            \"content\": {\"type\": \"text\"},\n",
        "            \"content_vector\": {\n",
        "                \"type\": \"dense_vector\",\n",
        "                \"dims\": 768,\n",
        "                \"index\": True,\n",
        "                \"similarity\": \"cosine\"\n",
        "            },\n",
        "            \"title_description_vector\": {\n",
        "                \"type\": \"dense_vector\",\n",
        "                \"dims\": 768,\n",
        "                \"index\": True,\n",
        "                \"similarity\": \"cosine\"\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "index_name = \"relationship_consult\"\n",
        "\n",
        "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
        "es_client.indices.create(index=index_name, body=index_settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "relationship_df = pd.DataFrame([{'source': d.metadata['source'], 'title': d.metadata.get('title'), 'description': d.metadata.get('description',''),'content': d.page_content,'section':'relationship'} for d in dating_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "psy_df = pd.DataFrame([{'source': d.metadata['source'], 'title': d.metadata.get('title'), 'description': d.metadata.get('description',''),'content': d.page_content,'section':'psycology'} for d in psy_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_df = pd.concat([relationship_df, psy_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_df = all_df.drop_duplicates(subset=['title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents=[]\n",
        "for index, row in tqdm(all_df.iterrows()):\n",
        "    documents.append({\"source\": row['source'], \n",
        "                      \"title\": row[\"title\"],\n",
        "                      \"description\": row[\"description\"],\n",
        "                      \"content\": row[\"content\"],\n",
        "                      \"section\": row[\"section\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
        "model = SentenceTransformer(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for doc in tqdm(documents):\n",
        "    content = doc['content']\n",
        "    title = doc['title']\n",
        "    description = doc['description']\n",
        "    td = f\"Title:{title}\\nDescription:{description}\"\n",
        "\n",
        "    doc['content_vector'] = model.encode(content)\n",
        "    doc['title_description_vector'] = model.encode(td)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for doc in tqdm(documents):\n",
        "    es_client.index(index=index_name, document=doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"I broke up with my girlfriend yesterday\"\n",
        "v_q = model.encode(query)\n",
        "knn_query = {\n",
        "    \"field\": \"content_vector\",\n",
        "    \"query_vector\": v_q,\n",
        "    \"k\": 3,\n",
        "    \"num_candidates\": 10000,\n",
        "    \"boost\": 0.5,\n",
        "}\n",
        "keyword_query = {\n",
        "    \"bool\": {\n",
        "        \"must\": {\n",
        "            \"multi_match\": {\n",
        "                \"query\": query,\n",
        "                \"fields\": [\"content\", \"title\",\"description\"],\n",
        "                \"type\": \"best_fields\",\n",
        "                \"boost\": 0.5,\n",
        "            }\n",
        "        },\n",
        "    }\n",
        "}\n",
        "response = es_client.search(\n",
        "    index=index_name,\n",
        "    query=keyword_query,\n",
        "    knn=knn_query,\n",
        "    size=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for hit in response['hits']['hits']:\n",
        "    id = hit['_id']\n",
        "    score = hit['_score']\n",
        "    title = hit['_source']['title']\n",
        "    text = hit['_source']['content']\n",
        "    pretty_output = (f\"\\nID: {id}\\nTitle: {title}\\nContent: {text}\\nScore: {score}\")\n",
        "    print(pretty_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing with Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/rz/0gz_h6gx0cngzvm1_ml4k1lh0000gn/T/ipykernel_3153/1322687464.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  embeddings = SentenceTransformerEmbeddings(model_name=model_name)\n",
            "/Users/ppathorn/Documents/GitHub/llm_zoomcamp_2024_project/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "es_url = 'http://localhost:9200'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hybrid_query(query: str) -> Dict:\n",
        "    vector = embeddings.embed_query(query)  # same embeddings as for indexing\n",
        "    return {\n",
        "        \"query\": {\n",
        "            \"bool\": {\n",
        "                \"must\": {\n",
        "                    \"multi_match\": {\n",
        "                        \"query\": query,\n",
        "                        \"fields\": [\"content\", \"title\",\"description\"],\n",
        "                        \"type\": \"best_fields\",\n",
        "                        \"boost\": 0.5,\n",
        "                    }\n",
        "                },\n",
        "            }\n",
        "        },\n",
        "        \"knn\": {\n",
        "            \"field\": \"content_vector\",\n",
        "            \"query_vector\": vector,\n",
        "            \"k\": 3,\n",
        "            \"num_candidates\": 10000,\n",
        "            \"boost\": 0.5,\n",
        "        },\n",
        "        \"size\": 3,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_name = \"relationship_consult\"\n",
        "hybrid_retriever = ElasticsearchRetriever.from_es_params(\n",
        "    index_name=index_name,\n",
        "    body_func=hybrid_query,\n",
        "    content_field='content',\n",
        "    url=es_url,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ทำความรู้จักกับ โรคซึมเศร้า โดยจิตแพทย์ - Alljit Blog 2.001563\n",
            "แฟนป่วยโรคซึมเศร้า ส่วนเราเอาไงดี? รับมืออย่างไร - Alljit Blog 1.9367619\n",
            "มีแฟนแต่รู้สึกเหงา นักจิตวิทยามองว่าอย่างไร? - Alljit Blog 0.36788744\n"
          ]
        }
      ],
      "source": [
        "query = \"เศร้า\"\n",
        "hybrid_results = hybrid_retriever.invoke(query)\n",
        "for result in hybrid_results:\n",
        "    print(result.metadata['_source']['title'], result.metadata['_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def elastic_search(query):\n",
        "    return hybrid_retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_prompt(query, search_results):\n",
        "    prompt_template = \"\"\"\n",
        "You are female relationship counselor name Saddie. Answer the QUESTION based on the CONTEXT with empathy.\n",
        "Use only the facts from the CONTEXT when answering the QUESTION.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "CONTEXT: \n",
        "{context}\n",
        "\"\"\".strip()\n",
        "\n",
        "    context = \"\"\n",
        "    \n",
        "    for doc in search_results:\n",
        "        context = context + f\"Title: {doc.metadata['_source']['title']}\\nDescription: {doc.metadata['_source']['description']}\\nContent: {doc.page_content}\\n\\n\"\n",
        "    \n",
        "    prompt = prompt_template.format(question=query, context=context).strip()\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## With LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse.decorators import observe\n",
        "from langfuse.callback import CallbackHandler\n",
        "from langfuse import Langfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "langfuse_handler = CallbackHandler(\n",
        "  secret_key=\"sk-lf-566804f8-f5cb-48d7-a4b0-65a9bc0b8c83\",\n",
        "  public_key=\"pk-lf-f1703f17-7916-4954-8907-ec3de4a17ca9\",\n",
        "  host=\"http://localhost:3000\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_key='xxxx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    safety_settings={\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    },\n",
        "    top_p=0.9,           \n",
        "    top_k=50,          \n",
        "    temperature=0.7,      \n",
        "    google_api_key=api_key,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "@observe()\n",
        "def rag(query):\n",
        "    search_results = elastic_search(query)\n",
        "    prompt = build_prompt(query, search_results)\n",
        "    answer = llm(prompt)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = '''\n",
        "You are Saddie a relationship advisor chatbot. Your role is to provide thoughtful, empathetic, and actionable advice to users based on the given context. You will receive a variable `{context}` that provides important information about the user's situation or emotional state. When responding:\n",
        "\n",
        "1. **Empathy First**: Always approach each situation with understanding, compassion, and without judgment, considering the emotional tone provided in `{context}`.\n",
        "2. **Tailor Responses**: Use the details in `{context}` to customize your advice. Whether the user is feeling hurt, confused, or happy, adjust your tone and suggestions accordingly.\n",
        "3. **Balanced Advice**: Provide balanced perspectives, considering both emotional and practical aspects of relationship dynamics. Always factor in the specific details of `{context}`.\n",
        "4. **Clarity**: Keep responses clear, concise, and free from jargon. Ensure your advice is actionable and suited to the user's situation as described in `{context}`.\n",
        "5. **Non-Biased**: Avoid taking sides in conflicts; instead, focus on encouraging healthy communication, mutual respect, and personal growth. Be sensitive to any biases or specific issues mentioned in `{context}`.\n",
        "6. **Emotionally Supportive**: Be positive, uplifting, and sensitive to the emotions involved in the conversation, as described in `{context}`.\n",
        "7. **Encourage Communication**: When appropriate, remind users of the importance of open and honest communication with their partners, adjusting advice based on their specific needs from `{context}`.\n",
        "8. **Resource Suggestion**: In cases where external help might be useful (such as therapy or professional consultation), gently suggest these resources, especially if `{context}` suggests a need for deeper intervention.\n",
        "\n",
        "You should be respectful of all types of relationships and inclusive of different genders, orientations, and cultural backgrounds. Always adapt your tone to the user's emotional state, as indicated by `{context}`, providing comfort and support where necessary.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(hybrid_retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, hybrid_retriever, contextualize_q_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "\n",
        "conversational_rag_chain = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(conversational_rag_chain.invoke(\n",
        "    {\"input\": \"ฉันเศร้า\"},\n",
        "    config={\n",
        "        \"configurable\": {\"session_id\": \"abc\"},\n",
        "        \"callbacks\": [langfuse_handler]\n",
        "    },  \n",
        ")[\"answer\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
